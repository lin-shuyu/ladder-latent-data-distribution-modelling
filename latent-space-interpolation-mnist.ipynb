{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Interpolation Demo (MNIST digit)\n",
    "- In this notebook, we will load a pre-trained LaDDer model for MNIST digit dataset. \n",
    "- We will then interpolate between a pair of MNIST images over k=5 steps, using the following two methods:\n",
    "  - shortest path (SP) method, which linearly interpolates between the two images in the latent space\n",
    "  - shortest likelihood path (SLP) method (**our proposal**), which considers the derived data manifold in the latent space and only interpolates samples with high likelihood.  \n",
    "- We will visualise the interpolated images in the 2D latent space between the two methods to show the difference in the interpolated paths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you select some hyper-parameters for the interpolation task\n",
    "- initialised_method: we initialise the path as a linear interpolation \n",
    "- n_step: the number of interpolated images to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_method = 'straight_line'\n",
    "n_step = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import plot, ion, show, savefig, cla, figure\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib._color_data as mcd\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"..\")\n",
    "\n",
    "from codes.utils import process_config, create_dirs, get_args, count_trainable_variables\n",
    "from codes.data_loader import DataGenerator\n",
    "from codes.models import MNISTModel_digit, MNISTModel_fashion, CelebAModel_densenet\n",
    "from codes.trainers import MNISTTrainer_joint_training, CelebATrainer_joint_training\n",
    "from demo.demo_tools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpolation demo does **not** require GPU to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(\"Available device is {}\".format(get_available_gpus()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained model\n",
    "We provided pre-trained models for all 3 datasets in `../pretrained_models/` folder.\n",
    "\n",
    "Please specify the dataset name (in the variable `exp_name` below) of which you want to load the corresponding model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the dataset that you want to interpolate\n",
    "# the current code only works with mnist datasets.\n",
    "# the demo for celebA dataset will be created in another demo. \n",
    "exp_name = 'mnist_digit' # 'mnist_fashion' or 'celebA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = process_config('demo/{}_config.json'.format(exp_name))\n",
    "\n",
    "# create the experiments dirs\n",
    "create_dirs([config['result_dir'], config['checkpoint_dir']])\n",
    "\n",
    "# create tensorflow session\n",
    "sess_config = tf.ConfigProto()\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=sess_config)\n",
    "\n",
    "# create your data generator\n",
    "data = DataGenerator(config, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the laDDer model\n",
    "if config['exp_name'] == \"mnist_digit\":\n",
    "    model = MNISTModel_digit(config)\n",
    "elif config['exp_name'] == \"mnist_fashion\":\n",
    "    model = MNISTModel_fashion(config)\n",
    "elif config['exp_name'] == \"celeba\":\n",
    "    model = CelebAModel_densenet(config)\n",
    "\n",
    "# create a trainer\n",
    "if config['exp_name'] in [\"mnist_digit\", \"mnist_fashion\"]:\n",
    "    trainer = MNISTTrainer_joint_training(sess, model, data, config)\n",
    "    sess.run(model.iterator.initializer,\n",
    "             feed_dict={model.original_signal: data.train_set['image'],\n",
    "                        model.seed: trainer.cur_epoch})\n",
    "elif config['exp_name'] == \"celeba\":\n",
    "    trainer = CelebATrainer_joint_training(sess, model, data, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load model if exists\n",
    "model.load(sess, model=\"VAE\")\n",
    "if config['prior'] in [\"ours\", 'hierarchical', 'vampPrior']:\n",
    "    model.load(sess, model=\"prior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we fit a GMM hyper prior for our model using 20k training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a GMM hyper prior \n",
    "if config['prior'] == \"ours\":\n",
    "    samples = trainer.fit_GMM_VI(iterator=model.input_image, mode=\"accurate\", space=\"t\")\n",
    "    if config['representation_size'] == 2:\n",
    "        trainer.plot_prior_distribution(samples, mode=\"accurate-GM\", style='circle', show_img=True)\n",
    "        trainer.plot_prior_distribution(samples, mode=\"accurate-GM\", style='density', show_img=True)\n",
    "elif config['prior'] == \"GMM\":\n",
    "    samples = trainer.fit_GMM_VI(iterator=model.input_image, mode=\"accurate\", space=\"z\")\n",
    "    if config['code_size'] == 2:\n",
    "        trainer.plot_prior_distribution(samples, mode=\"accurate-GM\", style='circle', show_img=True)\n",
    "        trainer.plot_prior_distribution(samples, mode=\"accurate-GM\", style='density', show_img=True)\n",
    "\n",
    "if config['prior'] in ['ours', 'GMM']:\n",
    "    GM = dict()\n",
    "    GM['m'] = (trainer.GM_prior_final.means_).astype(np.float32)\n",
    "    GM['K'] = (trainer.GM_prior_final.covariances_).astype(np.float32)\n",
    "    GM['w'] = (trainer.GM_prior_final.weights_).astype(np.float32)\n",
    "else:\n",
    "    GM = None\n",
    "\n",
    "prior = define_prior_distribution(config, sess, model, gmm_info=GM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here you select the two images that you want to interpolate between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get two query images - start and end of interpolation \n",
    "# please feel free to use different indices to try out different interpolation result \n",
    "save_dir = config['result_dir']\n",
    "\n",
    "idx_start = 0\n",
    "print(\"Start sample:\")\n",
    "embedding_start = get_embeddings_from_val_set(idx_start, config, exp_name, sess, data, model, trainer, \n",
    "                                              save_plot=False)\n",
    "\n",
    "idx_end = 32\n",
    "print(\"Target sample:\")\n",
    "embedding_end = get_embeddings_from_val_set(idx_end,  config, exp_name, sess, data, model, trainer, \n",
    "                                            save_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our shortest likelihood path interpolation starts here.\n",
    "- We first initialise the path using a linear path.\n",
    "- We then define the interpolation objective as shown in Eq (9) in our paper.\n",
    "- We then use AdamOptimizer to optimise the objective for 500 iterations. \n",
    "- The optimised embeddings correspond to the interpolated images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimisation objective \n",
    "# Generate a set of random embeddings as initialisation\n",
    "if initialise_method == 'random':\n",
    "    initialised_embeddings = generate_prior_embeddings(prior, n_embeddings=n_step)\n",
    "else: \n",
    "    initialised_embeddings = np.linspace(embedding_start, embedding_end, n_step+1, endpoint=False)[1:]\n",
    "# print(initialised_embeddings.shape)\n",
    "# print(initialised_embeddings[0])\n",
    "\n",
    "# Define tf.Variables for the 5 points in the intermediate steps\n",
    "with tf.variable_scope('interpolation'):\n",
    "    pts = tf.Variable(initialised_embeddings, dtype=tf.float32, trainable=True, name='intermediate_pts')\n",
    "\n",
    "# Compute the path length (Euclidean)\n",
    "pts_with_start = tf.concat([tf.expand_dims(embedding_start, 0), pts], axis=0)\n",
    "pts_with_end = tf.concat([pts, tf.expand_dims(embedding_end, 0)], axis=0)\n",
    "segment_length = tf.sqrt(tf.reduce_sum(tf.square(pts_with_end - pts_with_start), axis=1))\n",
    "equal_length_constraint = tf.math.reduce_std(segment_length)\n",
    "entire_path_length = tf.reduce_sum(segment_length)\n",
    "\n",
    "# Evaluate the likelihood along the path\n",
    "neg_ll = - tf.reduce_sum(prior.log_prob(pts))\n",
    "\n",
    "w_equal_length = tf.placeholder(tf.float32, [])\n",
    "w_path_dist = tf.placeholder(tf.float32, [])\n",
    "\n",
    "# Overall objective\n",
    "interpolation_obj = w_path_dist * entire_path_length + w_equal_length * equal_length_constraint + neg_ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_step_size = 100\n",
    "w_shortest_path = 10\n",
    "\n",
    "# Define optimisation step\n",
    "interpolation_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"interpolation\")\n",
    "num_interpolation = count_trainable_variables('interpolation')\n",
    "\n",
    "lr_interpolation = tf.placeholder(tf.float32, [])\n",
    "opt_interpolation = tf.train.AdamOptimizer(learning_rate=lr_interpolation,\n",
    "                                beta1=0.9,\n",
    "                                beta2=0.95)\n",
    "sess.run(tf.variables_initializer(opt_interpolation.variables()))\n",
    "gvs_interpolation = opt_interpolation.compute_gradients(interpolation_obj, var_list=interpolation_vars)\n",
    "print('gvs interpolation: {}'.format(gvs_interpolation))\n",
    "capped_gvs_interpolation = [(model.ClipIfNotNone(grad), var) for grad, var in gvs_interpolation]\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    update_step_interpolation = opt_interpolation.apply_gradients(capped_gvs_interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of random embeddings as initialisation\n",
    "if initialise_method == 'random':\n",
    "    initialised_embeddings = generate_prior_embeddings(prior, n_embeddings=n_step)\n",
    "pts.assign(initialised_embeddings)\n",
    "\n",
    "sess.run(pts.initializer)\n",
    "sess.run(tf.variables_initializer(opt_interpolation.variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimisation over certain iterations\n",
    "n_iter = 500\n",
    "cur_lr = 1e-2\n",
    "n_iter_print = 100\n",
    "\n",
    "loss = []\n",
    "pts_record = []\n",
    "step_var_record = []\n",
    "path_length_record = []\n",
    "neg_ll_record = []\n",
    "feed_dict = {lr_interpolation: cur_lr,\n",
    "             w_equal_length: w_step_size,\n",
    "             w_path_dist: w_shortest_path,\n",
    "             model.original_signal: trainer.test_batch}\n",
    "for i in range(n_iter):\n",
    "    cur_loss, cur_pts, cur_step_var, cur_path_length, cur_ll, _ = sess.run([interpolation_obj,\n",
    "                                     pts,\n",
    "                                     equal_length_constraint,\n",
    "                                     entire_path_length,\n",
    "                                     neg_ll,\n",
    "                                     update_step_interpolation], feed_dict=feed_dict)\n",
    "    loss.append(cur_loss)\n",
    "    pts_record.append(cur_pts)\n",
    "    step_var_record.append(cur_step_var)\n",
    "    path_length_record.append(cur_path_length)\n",
    "    neg_ll_record.append(cur_ll)\n",
    "    if i % n_iter_print == n_iter_print-1:\n",
    "        print(\"Iter {}/{}: cur loss is {}\".format(i+1, n_iter, cur_loss))\n",
    "print(\"Step variance is {}\".format(cur_step_var))\n",
    "\n",
    "print(\"Losses in our interpolation objective are optimised as below:\")\n",
    "plot_interpolation_losses(loss, path_length_record, step_var_record, neg_ll_record, n_iter,\n",
    "                          idx_start, idx_end, n_step, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we demonstrate the interpolation results\n",
    "- We first show the trace of interpolated images under our method.\n",
    "- We then visualise the interpolated path in the 2D latent space. This is compared with the linear path using shortest path (SP) method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the interpolated images from start image to target image\n",
    "print(\"SLP interpolation (ours)\")\n",
    "interpolated_embeddings = np.concatenate(\n",
    "    [np.expand_dims(embedding_start, 0), cur_pts, np.expand_dims(embedding_end, 0)], axis=0)\n",
    "plot_interpolated_images(interpolated_embeddings, config, exp_name, sess, data, model, trainer,\n",
    "                         n_step, idx_start, idx_end, save_plot=True, name_input='SLP')\n",
    "\n",
    "print(\"SP interpolation\")\n",
    "interpolated_embeddings = np.concatenate(\n",
    "    [np.expand_dims(embedding_start, 0), initialised_embeddings, np.expand_dims(embedding_end, 0)], axis=0)\n",
    "plot_interpolated_images(interpolated_embeddings, config, exp_name, sess, data, model, trainer,\n",
    "                         n_step, idx_start, idx_end, save_plot=True, name_input='SP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the interplated path in the 2D latent space for our SLP method\n",
    "plot_optimised_path(\n",
    "    cur_pts, config, GM, sess, model, trainer,\n",
    "    embedding_start, embedding_end, idx_start, idx_end, n_step, \n",
    "    prior=prior, \n",
    "    plot_prior='density', \n",
    "    w=2., \n",
    "    save_plot=True, \n",
    "    grid_size=7, \n",
    "    font_size=16,\n",
    "    name_input='SLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the interplated path in the 2D latent space for the linear SP method\n",
    "plot_optimised_path(\n",
    "    initialised_embeddings, config, GM, sess, model, trainer,\n",
    "    embedding_start, embedding_end, idx_start, idx_end, n_step, \n",
    "    prior=prior, \n",
    "    plot_prior='density', \n",
    "    w=2., \n",
    "    save_plot=True, \n",
    "    grid_size=7., \n",
    "    font_size=16, \n",
    "    name_input='SP', \n",
    "    c='lightblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ladder-env",
   "language": "python",
   "name": "ladder-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
